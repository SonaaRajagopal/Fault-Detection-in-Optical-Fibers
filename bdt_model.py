# -*- coding: utf-8 -*-
"""BDT_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qC5TOtvdbfdnYZ-sqIOefsKFV414SmIB
"""

!pip install pandas numpy scikit-learn lightgbm matplotlib seaborn

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, mean_squared_error, r2_score
from lightgbm import LGBMClassifier, LGBMRegressor
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns

# Load and preprocess data
otdr_df = pd.read_csv("/content/OTDR_data.csv").drop(columns=["Unnamed: 0"], errors='ignore')
speckle_df = pd.read_csv("/content/predictions._optical.csv")
speckle_temp = speckle_df['PredictedTemperature']
speckle_resampled = np.interp(np.linspace(0, len(speckle_temp)-1, len(otdr_df)), np.arange(len(speckle_temp)), speckle_temp)
otdr_df['Speckle_Temp'] = speckle_resampled

# Prepare features
signal_cols = [f'P{i}' for i in range(1, 31)]
other_features = ['SNR', 'Speckle_Temp']
X_seq = otdr_df[signal_cols].values
X_tab = otdr_df[other_features].values
y_class = otdr_df['Class'].values
y_pos = otdr_df['Position'].values
y_loss = otdr_df['loss'].values

# Train/test split
X_seq_train, X_seq_test, X_tab_train, X_tab_test, y_class_train, y_class_test, y_pos_train, y_pos_test, y_loss_train, y_loss_test = train_test_split(
    X_seq, X_tab, y_class, y_pos, y_loss, test_size=0.2, random_state=42
)

# ----------------- LSTM Module -----------------
class LSTMEncoder(nn.Module):
    def __init__(self, input_size=1, hidden_size=32, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.flatten = nn.Flatten()

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.flatten(out[:, -1:, :])  # Last timestep

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
lstm_model = LSTMEncoder().to(device)

# Prepare data for LSTM
X_seq_train_torch = torch.tensor(X_seq_train[:, :, None], dtype=torch.float32).to(device)
X_seq_test_torch = torch.tensor(X_seq_test[:, :, None], dtype=torch.float32).to(device)

# Training loop for LSTM as feature extractor
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)
epochs = 5

for epoch in range(epochs):
    lstm_model.train()
    outputs = lstm_model(X_seq_train_torch)
    loss = criterion(outputs, torch.tensor(y_pos_train[:, None], dtype=torch.float32).to(device))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Extract LSTM embeddings
lstm_model.eval()
with torch.no_grad():
    train_embed = lstm_model(X_seq_train_torch).cpu().numpy()
    test_embed = lstm_model(X_seq_test_torch).cpu().numpy()

# Combine LSTM embeddings with other tabular features
X_train_combined = np.hstack([train_embed, X_tab_train])
X_test_combined = np.hstack([test_embed, X_tab_test])

# ---------------- LightGBM Training ----------------
clf = LGBMClassifier()
clf.fit(X_train_combined, y_class_train)
y_class_pred = clf.predict(X_test_combined)

reg_pos = LGBMRegressor()
reg_pos.fit(X_train_combined, y_pos_train)
y_pos_pred = reg_pos.predict(X_test_combined)

reg_loss = LGBMRegressor()
reg_loss.fit(X_train_combined, y_loss_train)
y_loss_pred = reg_loss.predict(X_test_combined)

# ---------------- Evaluation ----------------
print("\n--- Classification ---")
print(classification_report(y_class_test, y_class_pred))

print("\n--- Position Regression ---")
print(f"RMSE: {np.sqrt(mean_squared_error(y_pos_test, y_pos_pred)):.4f}")
print(f"R²: {r2_score(y_pos_test, y_pos_pred):.4f}")

print("\n--- Loss Regression ---")
print(f"RMSE: {np.sqrt(mean_squared_error(y_loss_test, y_loss_pred)):.4f}")
print(f"R²: {r2_score(y_loss_test, y_loss_pred):.4f}")

# ---------------- Visualizations ----------------
plt.figure(figsize=(14, 5))

# Classification Confusion Matrix
plt.subplot(1, 3, 1)
sns.heatmap(pd.crosstab(y_class_test, y_class_pred), annot=True, fmt="d", cmap="Blues")
plt.title("Fault Type Prediction")
plt.xlabel("Predicted")
plt.ylabel("Actual")

# Position Prediction
plt.subplot(1, 3, 2)
plt.scatter(y_pos_test, y_pos_pred, alpha=0.3)
plt.plot([y_pos_test.min(), y_pos_test.max()], [y_pos_test.min(), y_pos_test.max()], 'r--')
plt.title("Position Prediction")
plt.xlabel("True Position")
plt.ylabel("Predicted")

# Loss Prediction
plt.subplot(1, 3, 3)
plt.scatter(y_loss_test, y_loss_pred, alpha=0.3, color='green')
plt.plot([y_loss_test.min(), y_loss_test.max()], [y_loss_test.min(), y_loss_test.max()], 'r--')
plt.title("Loss Prediction")
plt.xlabel("True Loss")
plt.ylabel("Predicted")

plt.tight_layout()
plt.show()